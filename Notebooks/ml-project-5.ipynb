{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf721c78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:36.401816Z",
     "iopub.status.busy": "2024-11-27T10:24:36.401057Z",
     "iopub.status.idle": "2024-11-27T10:24:36.993452Z",
     "shell.execute_reply": "2024-11-27T10:24:36.992423Z"
    },
    "papermill": {
     "duration": 0.59959,
     "end_time": "2024-11-27T10:24:36.995251",
     "exception": false,
     "start_time": "2024-11-27T10:24:36.395661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/brats20-dataset-training-validation\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"awsaf49/brats20-dataset-training-validation\")\n",
    "print(\"Path to dataset files:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d024845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:37.003101Z",
     "iopub.status.busy": "2024-11-27T10:24:37.002817Z",
     "iopub.status.idle": "2024-11-27T10:24:43.750807Z",
     "shell.execute_reply": "2024-11-27T10:24:43.749716Z"
    },
    "papermill": {
     "duration": 6.753903,
     "end_time": "2024-11-27T10:24:43.752668",
     "exception": false,
     "start_time": "2024-11-27T10:24:36.998765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"Random seed set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b51131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.760888Z",
     "iopub.status.busy": "2024-11-27T10:24:43.760248Z",
     "iopub.status.idle": "2024-11-27T10:24:43.768521Z",
     "shell.execute_reply": "2024-11-27T10:24:43.767685Z"
    },
    "papermill": {
     "duration": 0.014216,
     "end_time": "2024-11-27T10:24:43.770307",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.756091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MRISegDataset(Dataset):\n",
    "    def __init__(self, base_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_path (str): Path to the dataset folder.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.patients = [p for p in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, p))]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patients)*155\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the patient to fetch data for.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing 'image', 'segmentation', and optionally 'patient_id'.\n",
    "        \"\"\"\n",
    "        patient_id = self.patients[ind//155]\n",
    "        if patient_id == 'BraTS20_Training_355': \n",
    "            flair_path = os.path.join(self.base_path, patient_id, \"W39_1998.09.19_Segm.nii\")\n",
    "            seg_path = os.path.join(self.base_path, patient_id, \"W39_1998.09.19_Segm.nii\")\n",
    "        else:\n",
    "            flair_path = os.path.join(self.base_path, patient_id, f\"{patient_id}_flair.nii\")\n",
    "            seg_path = os.path.join(self.base_path, patient_id, f\"{patient_id}_seg.nii\")\n",
    "\n",
    "        flair = nib.load(flair_path).get_fdata()[:, :, ind%155].reshape(1, 240, 240)\n",
    "        seg = nib.load(seg_path).get_fdata()[:, :, ind%155].reshape(1, 240, 240)\n",
    "        seg = np.where(seg==4, 3, seg)\n",
    "\n",
    "        # Normalization\n",
    "        flair = (flair - 0) / (900 - 0)\n",
    "\n",
    "        flair = torch.tensor(flair, dtype=torch.float32)\n",
    "        seg = torch.tensor(seg, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            flair = self.transform(flair)\n",
    "            seg = self.transform(seg)\n",
    "\n",
    "        sample = {'image': flair, 'segmentation': seg, 'patient_id': patient_id}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aece41b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.778684Z",
     "iopub.status.busy": "2024-11-27T10:24:43.778434Z",
     "iopub.status.idle": "2024-11-27T10:24:43.783746Z",
     "shell.execute_reply": "2024-11-27T10:24:43.782918Z"
    },
    "papermill": {
     "duration": 0.011858,
     "end_time": "2024-11-27T10:24:43.785405",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.773547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    pred_class_all = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    dice_total = []\n",
    "    for class_ind in range(4):\n",
    "        pred_class = (pred_class_all == class_ind).float()\n",
    "        target_class = (target == class_ind).float()\n",
    "        \n",
    "        intersection = (pred_class * target_class).sum(dim=(1,2))\n",
    "        union = pred_class.sum(dim=(1,2)) + target_class.sum(dim=(1,2))\n",
    "        \n",
    "        dice_class = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_total.append(dice_class)\n",
    "    \n",
    "    return torch.mean(torch.stack(dice_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3130a64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.792836Z",
     "iopub.status.busy": "2024-11-27T10:24:43.792592Z",
     "iopub.status.idle": "2024-11-27T10:24:43.796846Z",
     "shell.execute_reply": "2024-11-27T10:24:43.796007Z"
    },
    "papermill": {
     "duration": 0.009939,
     "end_time": "2024-11-27T10:24:43.798516",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.788577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Currently using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b984968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.806002Z",
     "iopub.status.busy": "2024-11-27T10:24:43.805726Z",
     "iopub.status.idle": "2024-11-27T10:24:43.836881Z",
     "shell.execute_reply": "2024-11-27T10:24:43.835951Z"
    },
    "papermill": {
     "duration": 0.03665,
     "end_time": "2024-11-27T10:24:43.838460",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.801810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreFilter(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Total trainable parameters: 4,148\n"
     ]
    }
   ],
   "source": [
    "class PreFilter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreFilter, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        out = self.conv4(x)\n",
    "        return out\n",
    "        \n",
    "tmp_model = PreFilter()\n",
    "\n",
    "print(tmp_model)\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in tmp_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef37e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.845857Z",
     "iopub.status.busy": "2024-11-27T10:24:43.845639Z",
     "iopub.status.idle": "2024-11-27T10:24:43.860186Z",
     "shell.execute_reply": "2024-11-27T10:24:43.859194Z"
    },
    "papermill": {
     "duration": 0.019901,
     "end_time": "2024-11-27T10:24:43.861713",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.841812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet_Res(\n",
      "  (enc1): ResBlock(\n",
      "    (conv1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (skip): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (enc2): ResBlock(\n",
      "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (skip): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): ResBlock(\n",
      "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (up2): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (dec2): ResBlock(\n",
      "    (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (skip): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (up1): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (dec1): ResBlock(\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (skip): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (final_conv): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Total trainable parameters: 80,180\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)  # Match dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = torch.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += identity # Reisdual connection\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet_Res(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet_Res, self).__init__()\n",
    "\n",
    "        self.enc1 = ResBlock(2, 16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = ResBlock(16, 32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = ResBlock(32, 32)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2)\n",
    "        self.dec2 = ResBlock(64, 32)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.dec1 = ResBlock(32, 16)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, seg_mask):\n",
    "        # Encoder\n",
    "        x = torch.cat([x,seg_mask], dim=1)\n",
    "        enc1 = self.enc1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.enc2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "\n",
    "        bottleneck = self.bottleneck(enc2_pool)\n",
    "\n",
    "        up2 = self.up2(bottleneck)\n",
    "        dec2 = self.dec2(torch.cat([up2, enc2], dim=1))\n",
    "\n",
    "        up1 = self.up1(dec2)\n",
    "        dec1 = self.dec1(torch.cat([up1, enc1], dim=1))\n",
    "\n",
    "        out = self.final_conv(dec1)\n",
    "        return out\n",
    "\n",
    "\n",
    "tmp_model = UNet_Res()\n",
    "\n",
    "print(tmp_model)\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in tmp_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a182878d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:43.869234Z",
     "iopub.status.busy": "2024-11-27T10:24:43.868995Z",
     "iopub.status.idle": "2024-11-27T10:24:44.572866Z",
     "shell.execute_reply": "2024-11-27T10:24:44.571943Z"
    },
    "papermill": {
     "duration": 0.709917,
     "end_time": "2024-11-27T10:24:44.574947",
     "exception": false,
     "start_time": "2024-11-27T10:24:43.865030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 64, 64])\n",
      "Segmentation shape: torch.Size([1, 64, 64])\n",
      "Patient ID: BraTS20_Training_083\n"
     ]
    }
   ],
   "source": [
    "base_path = dataset_path +'/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "brats_dataset = MRISegDataset(base_path=base_path, transform=transforms.Resize((64,64)))\n",
    "\n",
    "# Example: Access a single sample\n",
    "sample = brats_dataset[0]\n",
    "print(\"Image shape:\", sample['image'].shape)\n",
    "print(\"Segmentation shape:\", sample['segmentation'].shape)\n",
    "print(\"Patient ID:\", sample['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74df5f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:44.585997Z",
     "iopub.status.busy": "2024-11-27T10:24:44.585688Z",
     "iopub.status.idle": "2024-11-27T10:24:44.748940Z",
     "shell.execute_reply": "2024-11-27T10:24:44.747952Z"
    },
    "papermill": {
     "duration": 0.171012,
     "end_time": "2024-11-27T10:24:44.751044",
     "exception": false,
     "start_time": "2024-11-27T10:24:44.580032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 3575\n",
      "Moved to cuda.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "data_loader = DataLoader(brats_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                         num_workers=2)\n",
    "print(f\"Number of Batches: {len(data_loader)}\")\n",
    "\n",
    "model_filter = PreFilter()\n",
    "if torch.cuda.is_available(): \n",
    "    model_filter = model_filter.cuda()\n",
    "    print(\"Moved to cuda.\")\n",
    "criterion_filter = nn.CrossEntropyLoss()\n",
    "optimizer_filter = torch.optim.Adam(model_filter.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dca40b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:24:44.762715Z",
     "iopub.status.busy": "2024-11-27T10:24:44.761887Z",
     "iopub.status.idle": "2024-11-27T12:34:50.061979Z",
     "shell.execute_reply": "2024-11-27T12:34:50.060983Z"
    },
    "papermill": {
     "duration": 7805.314225,
     "end_time": "2024-11-27T12:34:50.070319",
     "exception": false,
     "start_time": "2024-11-27T10:24:44.756094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3575, Time: 1.1311, Loss: 1.4035\n",
      "Batch 300/3575, Time: 0.1672, Loss: 0.0848\n",
      "Batch 600/3575, Time: 0.2290, Loss: 0.0449\n",
      "Batch 900/3575, Time: 0.2626, Loss: 0.0282\n",
      "Batch 1200/3575, Time: 0.1739, Loss: 0.0749\n",
      "Batch 1500/3575, Time: 0.2530, Loss: 0.0509\n",
      "Batch 1800/3575, Time: 0.2593, Loss: 0.0589\n",
      "Batch 2100/3575, Time: 0.1718, Loss: 0.0849\n",
      "Batch 2400/3575, Time: 0.2660, Loss: 0.0197\n",
      "Batch 2700/3575, Time: 0.2683, Loss: 0.0703\n",
      "Batch 3000/3575, Time: 0.2694, Loss: 0.0487\n",
      "Batch 3300/3575, Time: 0.1873, Loss: 0.0394\n",
      "Epoch 1/5, Time: 1589.4520, Loss: 0.0604\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.3596, Loss: 0.0547\n",
      "Batch 300/3575, Time: 0.2699, Loss: 0.0202\n",
      "Batch 600/3575, Time: 0.2598, Loss: 0.0427\n",
      "Batch 900/3575, Time: 0.1719, Loss: 0.0181\n",
      "Batch 1200/3575, Time: 0.2025, Loss: 0.0343\n",
      "Batch 1500/3575, Time: 0.1693, Loss: 0.0342\n",
      "Batch 1800/3575, Time: 0.1739, Loss: 0.0450\n",
      "Batch 2100/3575, Time: 0.1998, Loss: 0.0293\n",
      "Batch 2400/3575, Time: 0.1715, Loss: 0.0474\n",
      "Batch 2700/3575, Time: 0.1685, Loss: 0.0520\n",
      "Batch 3000/3575, Time: 0.2683, Loss: 0.0340\n",
      "Batch 3300/3575, Time: 0.2572, Loss: 0.0655\n",
      "Epoch 2/5, Time: 1555.5269, Loss: 0.0445\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.2976, Loss: 0.0332\n",
      "Batch 300/3575, Time: 0.1760, Loss: 0.0341\n",
      "Batch 600/3575, Time: 0.2601, Loss: 0.0177\n",
      "Batch 900/3575, Time: 0.2734, Loss: 0.0669\n",
      "Batch 1200/3575, Time: 0.1662, Loss: 0.0354\n",
      "Batch 1500/3575, Time: 0.2572, Loss: 0.0446\n",
      "Batch 1800/3575, Time: 0.2645, Loss: 0.0285\n",
      "Batch 2100/3575, Time: 0.2143, Loss: 0.0727\n",
      "Batch 2400/3575, Time: 0.1942, Loss: 0.0227\n",
      "Batch 2700/3575, Time: 0.1674, Loss: 0.0473\n",
      "Batch 3000/3575, Time: 0.2633, Loss: 0.0514\n",
      "Batch 3300/3575, Time: 0.1692, Loss: 0.0633\n",
      "Epoch 3/5, Time: 1557.4161, Loss: 0.0412\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.4497, Loss: 0.0592\n",
      "Batch 300/3575, Time: 0.1708, Loss: 0.0312\n",
      "Batch 600/3575, Time: 0.2662, Loss: 0.0798\n",
      "Batch 900/3575, Time: 0.2050, Loss: 0.0291\n",
      "Batch 1200/3575, Time: 0.1691, Loss: 0.0796\n",
      "Batch 1500/3575, Time: 0.2763, Loss: 0.0464\n",
      "Batch 1800/3575, Time: 0.2235, Loss: 0.0238\n",
      "Batch 2100/3575, Time: 0.2658, Loss: 0.0437\n",
      "Batch 2400/3575, Time: 0.2805, Loss: 0.0492\n",
      "Batch 2700/3575, Time: 0.1731, Loss: 0.0477\n",
      "Batch 3000/3575, Time: 0.2702, Loss: 0.0677\n",
      "Batch 3300/3575, Time: 0.2733, Loss: 0.0250\n",
      "Epoch 4/5, Time: 1546.3114, Loss: 0.0399\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.3422, Loss: 0.0370\n",
      "Batch 300/3575, Time: 0.2786, Loss: 0.0376\n",
      "Batch 600/3575, Time: 0.2763, Loss: 0.0514\n",
      "Batch 900/3575, Time: 0.2706, Loss: 0.0653\n",
      "Batch 1200/3575, Time: 0.2696, Loss: 0.0305\n",
      "Batch 1500/3575, Time: 0.2267, Loss: 0.0547\n",
      "Batch 1800/3575, Time: 0.2379, Loss: 0.0787\n",
      "Batch 2100/3575, Time: 0.2213, Loss: 0.0176\n",
      "Batch 2400/3575, Time: 0.2683, Loss: 0.0292\n",
      "Batch 2700/3575, Time: 0.2719, Loss: 0.0175\n",
      "Batch 3000/3575, Time: 0.2769, Loss: 0.0294\n",
      "Batch 3300/3575, Time: 0.2249, Loss: 0.0474\n",
      "Epoch 5/5, Time: 1556.5742, Loss: 0.0388\n",
      "Saving weights\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "training_losses = []\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model_filter.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        batch_start_time = time.time()\n",
    "        images = batch['image'].to(device) \n",
    "        segmentations = batch['segmentation'].squeeze(1).to(device)\n",
    "\n",
    "        optimizer_filter.zero_grad()\n",
    "    \n",
    "        outputs = model_filter(images)\n",
    "        loss = criterion_filter(outputs, segmentations)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_filter.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        del images\n",
    "        del segmentations\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        if batch_idx%300 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(data_loader)}, Time: {time.time()-batch_start_time:.4f}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    epoch_time = time.time()-epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.4f}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Saving weights\")\n",
    "    torch.save(model_filter.state_dict(), f\"Filter_weights_{epoch}.pth\")\n",
    "    training_losses.append(avg_loss)\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "with open(\"Filter.txt\", \"w\") as f:\n",
    "    f.write(\"Training_losses: \\n\")\n",
    "    for i in training_losses: f.write(str(i)+\"\\n\")\n",
    "    f.write(\"\\nEpoch_times: \\n\")\n",
    "    for i in epoch_times: f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbfa2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T12:34:50.083996Z",
     "iopub.status.busy": "2024-11-27T12:34:50.083693Z",
     "iopub.status.idle": "2024-11-27T12:34:50.087751Z",
     "shell.execute_reply": "2024-11-27T12:34:50.086953Z"
    },
    "papermill": {
     "duration": 0.012834,
     "end_time": "2024-11-27T12:34:50.089300",
     "exception": false,
     "start_time": "2024-11-27T12:34:50.076466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model_filter.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b07802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T12:34:50.102728Z",
     "iopub.status.busy": "2024-11-27T12:34:50.102273Z",
     "iopub.status.idle": "2024-11-27T12:34:50.181626Z",
     "shell.execute_reply": "2024-11-27T12:34:50.180721Z"
    },
    "papermill": {
     "duration": 0.088296,
     "end_time": "2024-11-27T12:34:50.183647",
     "exception": false,
     "start_time": "2024-11-27T12:34:50.095351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 256, 256])\n",
      "Segmentation shape: torch.Size([1, 256, 256])\n",
      "Patient ID: BraTS20_Training_083\n"
     ]
    }
   ],
   "source": [
    "base_path = dataset_path +'/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "brats_dataset = MRISegDataset(base_path=base_path, transform=transforms.Resize((256,256)))\n",
    "\n",
    "# Example: Access a single sample\n",
    "sample = brats_dataset[0]\n",
    "print(\"Image shape:\", sample['image'].shape)\n",
    "print(\"Segmentation shape:\", sample['segmentation'].shape)\n",
    "print(\"Patient ID:\", sample['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "361f945b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T12:34:50.201748Z",
     "iopub.status.busy": "2024-11-27T12:34:50.201464Z",
     "iopub.status.idle": "2024-11-27T12:34:50.213236Z",
     "shell.execute_reply": "2024-11-27T12:34:50.212401Z"
    },
    "papermill": {
     "duration": 0.022291,
     "end_time": "2024-11-27T12:34:50.215062",
     "exception": false,
     "start_time": "2024-11-27T12:34:50.192771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 3575\n",
      "Moved to cuda.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "data_loader = DataLoader(brats_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                         num_workers=2)\n",
    "print(f\"Number of Batches: {len(data_loader)}\")\n",
    "\n",
    "model_res = UNet_Res()\n",
    "if torch.cuda.is_available(): \n",
    "    model_res = model_res.cuda()\n",
    "    \n",
    "    print(\"Moved to cuda.\")\n",
    "criterion_res = nn.CrossEntropyLoss()\n",
    "optimizer_res = torch.optim.Adam(model_res.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9bf31d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T12:34:50.232468Z",
     "iopub.status.busy": "2024-11-27T12:34:50.232213Z",
     "iopub.status.idle": "2024-11-27T14:54:51.136797Z",
     "shell.execute_reply": "2024-11-27T14:54:51.135778Z"
    },
    "papermill": {
     "duration": 8400.92435,
     "end_time": "2024-11-27T14:54:51.148222",
     "exception": false,
     "start_time": "2024-11-27T12:34:50.223872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3575, Time: 0.6443, Loss: 1.5043\n",
      "Batch 300/3575, Time: 0.3029, Loss: 0.0674\n",
      "Batch 600/3575, Time: 0.3589, Loss: 0.0653\n",
      "Batch 900/3575, Time: 0.3710, Loss: 0.0162\n",
      "Batch 1200/3575, Time: 0.2736, Loss: 0.0281\n",
      "Batch 1500/3575, Time: 0.3671, Loss: 0.0515\n",
      "Batch 1800/3575, Time: 0.2672, Loss: 0.0263\n",
      "Batch 2100/3575, Time: 0.2963, Loss: 0.0509\n",
      "Batch 2400/3575, Time: 0.3711, Loss: 0.0216\n",
      "Batch 2700/3575, Time: 0.2683, Loss: 0.0262\n",
      "Batch 3000/3575, Time: 0.3671, Loss: 0.0580\n",
      "Batch 3300/3575, Time: 0.3397, Loss: 0.0496\n",
      "Epoch 1/5, Time: 1684.0855, Loss: 0.0415\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.4525, Loss: 0.0282\n",
      "Batch 300/3575, Time: 0.2974, Loss: 0.0081\n",
      "Batch 600/3575, Time: 0.2708, Loss: 0.0622\n",
      "Batch 900/3575, Time: 0.3324, Loss: 0.0316\n",
      "Batch 1200/3575, Time: 0.3647, Loss: 0.0581\n",
      "Batch 1500/3575, Time: 0.3790, Loss: 0.0463\n",
      "Batch 1800/3575, Time: 0.3115, Loss: 0.0285\n",
      "Batch 2100/3575, Time: 0.3641, Loss: 0.0459\n",
      "Batch 2400/3575, Time: 0.3654, Loss: 0.0217\n",
      "Batch 2700/3575, Time: 0.2730, Loss: 0.0321\n",
      "Batch 3000/3575, Time: 0.3208, Loss: 0.0329\n",
      "Batch 3300/3575, Time: 0.3513, Loss: 0.0354\n",
      "Epoch 2/5, Time: 1677.4398, Loss: 0.0335\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.3832, Loss: 0.0579\n",
      "Batch 300/3575, Time: 0.2743, Loss: 0.0464\n",
      "Batch 600/3575, Time: 0.3067, Loss: 0.0471\n",
      "Batch 900/3575, Time: 0.3685, Loss: 0.0148\n",
      "Batch 1200/3575, Time: 0.3702, Loss: 0.0365\n",
      "Batch 1500/3575, Time: 0.3732, Loss: 0.0393\n",
      "Batch 1800/3575, Time: 0.3729, Loss: 0.0231\n",
      "Batch 2100/3575, Time: 0.3283, Loss: 0.0149\n",
      "Batch 2400/3575, Time: 0.3721, Loss: 0.0190\n",
      "Batch 2700/3575, Time: 0.3721, Loss: 0.0414\n",
      "Batch 3000/3575, Time: 0.3647, Loss: 0.0380\n",
      "Batch 3300/3575, Time: 0.2722, Loss: 0.0145\n",
      "Epoch 3/5, Time: 1674.3361, Loss: 0.0298\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.5434, Loss: 0.0299\n",
      "Batch 300/3575, Time: 0.3735, Loss: 0.0471\n",
      "Batch 600/3575, Time: 0.2689, Loss: 0.0272\n",
      "Batch 900/3575, Time: 0.2692, Loss: 0.0295\n",
      "Batch 1200/3575, Time: 0.3560, Loss: 0.0262\n",
      "Batch 1500/3575, Time: 0.2756, Loss: 0.0331\n",
      "Batch 1800/3575, Time: 0.2702, Loss: 0.0078\n",
      "Batch 2100/3575, Time: 0.3582, Loss: 0.0215\n",
      "Batch 2400/3575, Time: 0.3650, Loss: 0.0455\n",
      "Batch 2700/3575, Time: 0.3581, Loss: 0.0263\n",
      "Batch 3000/3575, Time: 0.3618, Loss: 0.0120\n",
      "Batch 3300/3575, Time: 0.3726, Loss: 0.0264\n",
      "Epoch 4/5, Time: 1690.5992, Loss: 0.0275\n",
      "Saving weights\n",
      "Batch 0/3575, Time: 0.5582, Loss: 0.0237\n",
      "Batch 300/3575, Time: 0.3432, Loss: 0.0335\n",
      "Batch 600/3575, Time: 0.3725, Loss: 0.0318\n",
      "Batch 900/3575, Time: 0.3619, Loss: 0.0406\n",
      "Batch 1200/3575, Time: 0.3058, Loss: 0.0207\n",
      "Batch 1500/3575, Time: 0.3726, Loss: 0.0417\n",
      "Batch 1800/3575, Time: 0.2698, Loss: 0.0347\n",
      "Batch 2100/3575, Time: 0.2709, Loss: 0.0383\n",
      "Batch 2400/3575, Time: 0.3060, Loss: 0.0209\n",
      "Batch 2700/3575, Time: 0.3639, Loss: 0.0128\n",
      "Batch 3000/3575, Time: 0.3210, Loss: 0.0366\n",
      "Batch 3300/3575, Time: 0.3734, Loss: 0.0126\n",
      "Epoch 5/5, Time: 1674.4125, Loss: 0.0262\n",
      "Saving weights\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "training_losses = []\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model_res.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        batch_start_time = time.time()\n",
    "        images = batch['image'].to(device) \n",
    "        segmentations = batch['segmentation'].squeeze(1).to(device)\n",
    "\n",
    "        optimizer_res.zero_grad()\n",
    "\n",
    "        seg_mask = nn.functional.interpolate(\n",
    "            model_filter(nn.functional.interpolate(images, size=(64, 64), mode='bilinear')),\n",
    "            size=(256, 256), \n",
    "            mode='bilinear'\n",
    "        )\n",
    "\n",
    "        seg_mask = torch.argmax(seg_mask, dim=1).unsqueeze(1)\n",
    "        outputs = model_res(images, seg_mask)\n",
    "        loss = criterion_res(outputs, segmentations)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_res.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        del images\n",
    "        del segmentations\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        if batch_idx%300 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(data_loader)}, Time: {time.time()-batch_start_time:.4f}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    epoch_time = time.time()-epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.4f}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Saving weights\")\n",
    "    torch.save(model_res.state_dict(), f\"UNet4_weights_{epoch}.pth\")\n",
    "    training_losses.append(avg_loss)\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "with open(\"UNet4.txt\", \"w\") as f:\n",
    "    f.write(\"Training_losses: \\n\")\n",
    "    for i in training_losses: f.write(str(i)+\"\\n\")\n",
    "    f.write(\"\\nEpoch_times: \\n\")\n",
    "    for i in epoch_times: f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442a850f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T14:54:51.167137Z",
     "iopub.status.busy": "2024-11-27T14:54:51.166817Z",
     "iopub.status.idle": "2024-11-27T14:54:51.173006Z",
     "shell.execute_reply": "2024-11-27T14:54:51.172313Z"
    },
    "papermill": {
     "duration": 0.017431,
     "end_time": "2024-11-27T14:54:51.174535",
     "exception": false,
     "start_time": "2024-11-27T14:54:51.157104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 256, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9a79d",
   "metadata": {
    "papermill": {
     "duration": 0.008524,
     "end_time": "2024-11-27T14:54:51.191813",
     "exception": false,
     "start_time": "2024-11-27T14:54:51.183289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57782a8",
   "metadata": {
    "papermill": {
     "duration": 0.008602,
     "end_time": "2024-11-27T14:54:51.209317",
     "exception": false,
     "start_time": "2024-11-27T14:54:51.200715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1331968,
     "datasetId": 751906,
     "sourceId": 1299795,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16218.878534,
   "end_time": "2024-11-27T14:54:52.841844",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T10:24:33.963310",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
